Practice 4 

Оптимизация параллельного кода на GPU с использованием различных типов памяти
Цель работы

Изучение и практическое применение различных типов памяти CUDA (глобальной, разделяемой и локальной) для оптимизации параллельных вычислений на графическом процессоре.

Теоретическая часть

В CUDA используются разные типы памяти, отличающиеся по скорости доступа и области применения. Глобальная память доступна всем потокам, но обладает высокой задержкой. Разделяемая память является быстрой и используется для обмена данными между потоками внутри одного блока. Локальная память предназначена для временных переменных отдельного потока и обычно размещается в регистрах.

Оптимизация CUDA-программ заключается в минимизации обращений к глобальной памяти, использовании разделяемой памяти для промежуточных вычислений и эффективной организации потоков. Такой подход позволяет существенно повысить производительность GPU-приложений.

Практическая часть
Задание 1. Подготовка данных

Была реализована программа генерации массива случайных чисел размером 1 000 000 элементов, размещаемого в глобальной памяти GPU.

Задание 2. Оптимизация параллельного редукционного алгоритма

Реализованы две версии алгоритма редукции суммы элементов массива: первая использует только глобальную память, вторая — комбинацию глобальной и разделяемой памяти. Проведено сравнение времени выполнения и показано, что использование разделяемой памяти значительно снижает число обращений к глобальной памяти и повышает производительность.

Задание 3. Оптимизация сортировки на GPU

Реализована сортировка пузырьком для небольших подмассивов с использованием локальной памяти потоков. Общий массив хранится в глобальной памяти, а операция слияния отсортированных подмассивов выполняется с применением разделяемой памяти для ускорения обмена данными между потоками блока.

Задание 4. Измерение производительности

Проведены замеры времени выполнения программ для массивов размером 10 000, 100 000 и 1 000 000 элементов. Построены графики зависимости времени выполнения от размера массива, демонстрирующие преимущество использования разделяемой и локальной памяти по сравнению с доступом только к глобальной памяти.

Контрольные вопросы

1. Чем отличаются типы памяти в CUDA и в каких случаях их использовать?
Глобальная память используется для хранения больших объёмов данных, разделяемая — для быстрого обмена между потоками одного блока, а локальная — для временных данных отдельного потока.

2. Как использование разделяемой памяти влияет на производительность?
Разделяемая память уменьшает количество обращений к глобальной памяти, что значительно снижает задержки и ускоряет выполнение программы.

3. Что такое коалесцированный доступ и как его обеспечить?
Коалесцированный доступ — это последовательный доступ потоков к соседним адресам глобальной памяти, который достигается правильной организацией данных и потоков.

4. Какие сложности возникают при работе с большим объёмом данных на GPU?
Основные сложности связаны с ограниченным объёмом памяти GPU, задержками при передаче данных между CPU и GPU и высокой стоимостью доступа к глобальной памяти.

5. Почему важно минимизировать доступ к глобальной памяти?
Поскольку глобальная память имеет большую задержку, частые обращения к ней существенно снижают производительность параллельных вычислений.

6. Как использовать профилирование для анализа производительности CUDA-программ?
Профилирование позволяет определить узкие места программы, измерить время выполнения ядер и операций копирования данных, а также оценить эффективность использования различных типов памяти.
